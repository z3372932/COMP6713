{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca75e98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-14 21:55:40,429] [WARNING] [real_accelerator.py:194:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-04-14 21:55:40,434] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0414 21:55:41.124699 5532 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import time\n",
    "import csv\n",
    "from huggingface_hub import notebook_login\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import sacrebleu\n",
    "from bert_score import score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8306fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84142c8ad5648c8a7dbd92317ce8781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e662a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load legal_t5 tokenizer and model\n",
    "\n",
    "# Replace with the model being used\n",
    "model_name = \"legal_t5\"\n",
    "model_id = \"SEBIS/legal_t5_small_trans_fr_en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca123a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "translation_pipeline = pipeline(\"translation_fr_to_en\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7616e4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'Tel: +31 (0)182 557 755', 'reference': 'Tel: +31 (0)182 557 755'}, {'input': '5.3 Données de sécurité précliniques Les données précliniques issues des études conventionnelles de pharmacologie de sécurité, toxicologie en administration répétée, génotoxicité, cancérogenèse, n’ ont pas révélé de risque particulier pour l’ homme.', 'reference': '5.3 Preclinical safety data Preclinical data revela no special hazard for humans based on conventional studies of safety pharmacology, repeated dose toxicity, genotoxicity and carcinogenic potential.'}, {'input': 'Le contenu de la seringue pré-remplie doit être secoué, puis administré par une injection sous-cutanée (sous la peau).', 'reference': 'The contents of the prefilled syringe are shaken and given by subcutaneous (under the skin) injection.'}, {'input': 'Si vous avez pris plus de Sebivo que vous n’ auriez dû Si vous avez pris trop de Sebivo ou si quelqu’ un d’ autre a pris votre solution buvable par erreur, consultez immédiatement votre médecin ou rendez-vous dans un hôpital.', 'reference': 'If you take more Sebivo than you should If you have taken too much Sebivo, or if someone else accidentally takes your oral solution, go to your doctor or hospital for advice straight away.'}, {'input': '6.4 Précautions particulières de conservation', 'reference': '6.4 Special precautions for storage'}]\n"
     ]
    }
   ],
   "source": [
    "# Open and read the .fr file\n",
    "with open(\"new_dataset/test.fr\", \"r\", encoding=\"utf-8\") as fr_file:\n",
    "    english_test_df = fr_file.readlines()\n",
    "\n",
    "# Open and read the .en file\n",
    "with open(\"new_dataset/test.en\", \"r\", encoding=\"utf-8\") as en_file:\n",
    "    french_test_df = en_file.readlines()\n",
    "\n",
    "# Combine into a list of dictionaries\n",
    "test_features = [{\"input\": fr.strip(), \"reference\": en.strip()} for fr, en in zip(french_test_df, english_test_df)]\n",
    "\n",
    "# Print a few examples\n",
    "print(test_features[:5])  # Display the first 5 dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ea45e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': '15.', 'reference': '15.'}, {'input': 'Autriche', 'reference': 'Austria'}, {'input': 'Lavez-vous soigneusement les mains.', 'reference': 'Wash your hands carefully.'}, {'input': 'Nom de fantaisie Dosage', 'reference': 'Invented Name'}, {'input': 'Minervum 7061 NL-4817 ZK Breda Pays-Bas', 'reference': 'Minervum 7061 NL-4817 ZK Breda The Netherlands'}, {'input': 'Le darunavir a montré une activité antivirale synergique lors de son association avec les inhibiteurs de protéase tels que ritonavir, nelfinavir ou amprénavir et une activité antivirale additive lors de son association avec les inhibiteurs de protéase: indinavir, saquinavir, lopinavir, atazanavir ou tipranavir, avec les INTIs: zidovudine, lamivudine, zalcitabine, didanosine, stavudine, abacavir, emtricitabine ou ténofovir, avec les INNTIs: néviparine, delavirdine ou efavirenz, ainsi qu’avec l’inhibiteur de fusion enfuvirtide.', 'reference': 'Darunavir showed synergistic antiviral activity when studied in combination with the protease inhibitors ritonavir, nelfinavir, or amprenavir and additive antiviral activity when studied in combination with the protease inhibitors indinavir, saquinavir, lopinavir, atazanavir, or tipranavir, the N(t)RTIs zidovudine, lamivudine, zalcitabine, didanosine, stavudine, abacavir, emtricitabine, or tenofovir, the NNRTIs nevirapine, delavirdine, or efavirenz and the fusion inhibitor enfuvirtide.'}, {'input': '[À compléter au niveau national]', 'reference': '[To be completed nationally]'}, {'input': '0.3 ml (500 µg/ ml)', 'reference': '0.3 ml (500 µg/ ml)'}, {'input': '1 Article 29, paragraphe 4, de la directive 2001/ 83/ CE, telle que modifiée.', 'reference': '1 Article 29(4) of Directive 2001/ 83/ EC, as amended.'}, {'input': '1.', 'reference': '1.'}]\n"
     ]
    }
   ],
   "source": [
    "print(test_features[930:940])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be4e821b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102071"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b979711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample No.: 20\n",
      "Elapsed Time: 0h 0m 13s\n",
      "ETA: 18h 45m 21s\n",
      "Input: • Règlement (CEE) n° 2309/93 du Conseil, du 22 juillet 1993 (JO L 214, 24.8.1993, p.\n",
      "Generated:  Council Regulation (EEC) No 2309/93 of 22 July 1993 (OJ L 214, 24.8.1993, p.\n",
      "Reference: • Council Regulation (EEC) No 2309/93 of 22 July 1993 (OJ L 214, 24.8.1993, p.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhours\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mh \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminutes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_features):\n\u001b[1;32m---> 20\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtranslation_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation_text\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     25\u001b[0m     }\n\u001b[0;32m     26\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:382\u001b[0m, in \u001b[0;36mTranslationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    Translate the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m          token ids of the translation.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:173\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[0;32m    178\u001b[0m     ):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\transformers\\pipelines\\base.py:1379\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1372\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1373\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         )\n\u001b[0;32m   1377\u001b[0m     )\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\transformers\\pipelines\\base.py:1386\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1385\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1386\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1387\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\transformers\\pipelines\\base.py:1286\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1285\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1286\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1287\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:202\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    200\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 202\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    203\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\transformers\\generation\\utils.py:2484\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[0;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2482\u001b[0m     )\n\u001b[0;32m   2483\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2484\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2485\u001b[0m         input_ids,\n\u001b[0;32m   2486\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2487\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2488\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2489\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2490\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2491\u001b[0m     )\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2494\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2495\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2496\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2497\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2503\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2504\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\transformers\\generation\\utils.py:3904\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3901\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3902\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m-> 3904\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3906\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3907\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3908\u001b[0m     model_outputs,\n\u001b[0;32m   3909\u001b[0m     model_kwargs,\n\u001b[0;32m   3910\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3911\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1934\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtie_word_embeddings:\n\u001b[0;32m   1930\u001b[0m     \u001b[38;5;66;03m# Rescale output before projecting on vocab\u001b[39;00m\n\u001b[0;32m   1931\u001b[0m     \u001b[38;5;66;03m# See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\u001b[39;00m\n\u001b[0;32m   1932\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m sequence_output \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m-> 1934\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1936\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\micha\\Documents\\Education\\Master of IT\\2025, T1\\COMP6713v3.10\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "# Define a CSV file to save the results\n",
    "output_csv_file = f\"{model_name}_translation_results.csv\"\n",
    "\n",
    "# Write the header to the CSV file (run this once at the beginning)\n",
    "with open(output_csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"input\", \"generated\", \"reference\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Convert seconds into hours, minutes, and seconds.\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{hours}h {minutes}m {seconds}s\"\n",
    "\n",
    "for i, sample in enumerate(test_features):\n",
    "    output = translation_pipeline(sample[\"input\"], max_length=512)\n",
    "    result = {\n",
    "        \"input\": sample[\"input\"],\n",
    "        \"generated\": output[0][\"translation_text\"],\n",
    "        \"reference\": sample[\"reference\"]\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    # Append to CSV every 10 samples\n",
    "    if i % 10 == 0:\n",
    "        # Calculate elapsed time and ETA\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_time_per_sample = elapsed_time / (i + 1)  # Avoid division by zero\n",
    "        remaining_samples = len(test_features) - (i + 1)\n",
    "        eta = avg_time_per_sample * remaining_samples\n",
    "\n",
    "        # Clear the cell output (works in Jupyter Notebook)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Display progress\n",
    "        print(f\"Sample No.: {i}\")\n",
    "        print(f\"Elapsed Time: {format_time(elapsed_time)}\")\n",
    "        print(f\"ETA: {format_time(eta)}\")\n",
    "        print(f\"Input: {sample['input']}\")\n",
    "        print(f\"Generated: {output[0]['translation_text']}\")\n",
    "        print(f\"Reference: {sample['reference']}\\n\")\n",
    "\n",
    "        # Save the current results to the CSV file\n",
    "        with open(output_csv_file, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"input\", \"generated\", \"reference\"])\n",
    "            writer.writerows(results)  # Append all new results\n",
    "        results = []  # Clear the results list after saving\n",
    "\n",
    "# Save any remaining results at the end\n",
    "if results:\n",
    "    with open(output_csv_file, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"input\", \"generated\", \"reference\"])\n",
    "        writer.writerows(results)\n",
    "\n",
    "print(f\"All results have been saved to {output_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769aaa4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>generated</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tel: +31 (0)182 557 755</td>\n",
       "      <td>Tel: +31 (0)182 557 755</td>\n",
       "      <td>Tel: +31 (0)182 557 755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3 Données de sécurité précliniques Les donné...</td>\n",
       "      <td>5.3 Pre-clinical safety data Preclinial data f...</td>\n",
       "      <td>5.3 Preclinical safety data Preclinical data r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0                            Tel: +31 (0)182 557 755   \n",
       "1  5.3 Données de sécurité précliniques Les donné...   \n",
       "\n",
       "                                           generated  \\\n",
       "0                            Tel: +31 (0)182 557 755   \n",
       "1  5.3 Pre-clinical safety data Preclinial data f...   \n",
       "\n",
       "                                           reference  \n",
       "0                            Tel: +31 (0)182 557 755  \n",
       "1  5.3 Preclinical safety data Preclinical data r...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(output_csv_file)\n",
    "results.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ebdca68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 41.69263275295573\n"
     ]
    }
   ],
   "source": [
    "# BLEU score\n",
    "results[\"generated\"] = results[\"generated\"].astype(str)\n",
    "results[\"reference\"] = results[\"reference\"].astype(str)\n",
    "\n",
    "# Extract the generated and reference translations\n",
    "generated = results[\"generated\"].tolist()\n",
    "references = [results[\"reference\"].tolist()]  # Wrap in a list as sacrebleu supports multiple references\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu = sacrebleu.corpus_bleu(generated, references)\n",
    "\n",
    "print(f\"BLEU score: {bleu.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea1ab3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall METEOR score: 0.5409643618058103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# METEOR score\n",
    "nltk.download('wordnet')\n",
    "\n",
    "results[\"generated\"] = results[\"generated\"].astype(str)\n",
    "results[\"reference\"] = results[\"reference\"].astype(str)\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    \"hypothesis\": results[\"generated\"],\n",
    "    \"reference\": results[\"reference\"]\n",
    "}\n",
    "combined_results = pd.DataFrame(data)\n",
    "\n",
    "# Tokenize the strings\n",
    "combined_results[\"hypothesis\"] = combined_results[\"hypothesis\"].apply(lambda x: x.split())\n",
    "combined_results[\"reference\"] = combined_results[\"reference\"].apply(lambda x: x.split())\n",
    "\n",
    "# Calculate METEOR scores for each row\n",
    "combined_results[\"meteor_score\"] = combined_results.apply(lambda row: meteor_score([row[\"reference\"]], row[\"hypothesis\"]), axis=1)\n",
    "\n",
    "overall_meteor_score = combined_results[\"meteor_score\"].mean()\n",
    "\n",
    "print(f\"Overall METEOR score: {overall_meteor_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea620c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9994ec25e45c42d78def8a6e5eb330df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08493d9af5764849b8041c8442c4fb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.60 seconds, 8.07 sentences/sec\n",
      "Overall BERTScore: 0.9437672978355771\n"
     ]
    }
   ],
   "source": [
    "# BERT score\n",
    "data = {\n",
    "    \"hypothesis\": results[\"generated\"],\n",
    "    \"reference\": results[\"reference\"]\n",
    "}\n",
    "combined_results = pd.DataFrame(data)\n",
    "\n",
    "# Extract the hypothesis and reference as lists\n",
    "hypotheses = combined_results[\"hypothesis\"].tolist()\n",
    "references = combined_results[\"reference\"].tolist()\n",
    "\n",
    "# Calculate BERTScore\n",
    "P, R, F1 = score(hypotheses, references, lang=\"en\", verbose=True)\n",
    "\n",
    "# Compute overall BERTScore (mean of F1 scores)\n",
    "overall_bert_score = sum(F1.tolist()) / len(F1)\n",
    "print(f\"Overall BERTScore: {overall_bert_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP6713v3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
